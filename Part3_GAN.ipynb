{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "$ \\newcommand{\\mat}[1]{\\boldsymbol {#1}} $\n",
    "$ \\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top} $\n",
    "$ \\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}} $\n",
    "$ \\newcommand{\\vec}[1]{\\boldsymbol {#1}} $\n",
    "$ \\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top} $\n",
    "$ \\newcommand{\\rvar}[1]{\\mathrm {#1}} $\n",
    "$ \\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}} $\n",
    "$ \\newcommand{\\diag}{\\mathop{\\mathrm {diag}}} $\n",
    "$ \\newcommand{\\set}[1]{\\mathbb {#1}} $\n",
    "$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} $\n",
    "$ \\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}} $\n",
    "$ \\newcommand{\\bm}[1]{{\\bf #1}} $\n",
    "$ \\newcommand{\\bb}[1]{\\bm{\\mathrm{#1}}} $\n",
    "\n",
    "# Part 3: Generative Adversarial Networks\n",
    "<a id=part3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will implement and train a generative adversarial network and apply it to the task of image generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:16.340375Z",
     "iopub.status.busy": "2022-03-19T14:31:16.340101Z",
     "iopub.status.idle": "2022-03-19T14:31:17.965570Z",
     "shell.execute_reply": "2022-03-19T14:31:17.964836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining the dataset\n",
    "<a id=part3_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same data as in Part 2.\n",
    "\n",
    "But again, you can use a custom dataset, by editing the `PART3_CUSTOM_DATA_URL` variable in `hw4/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:17.970436Z",
     "iopub.status.busy": "2022-03-19T14:31:17.970207Z",
     "iopub.status.idle": "2022-03-19T14:31:18.193556Z",
     "shell.execute_reply": "2022-03-19T14:31:18.193061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/Technion/.pytorch-datasets/lfw-bush.zip exists, skipping download.\n",
      "Extracting /Users/Technion/.pytorch-datasets/lfw-bush.zip...\n",
      "Extracted 531 to /Users/Technion/.pytorch-datasets/lfw/George_W_Bush\n"
     ]
    }
   ],
   "source": [
    "import cs236781.plot as plot\n",
    "import cs236781.download\n",
    "from hw4.answers import PART3_CUSTOM_DATA_URL as CUSTOM_DATA_URL\n",
    "\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "if CUSTOM_DATA_URL is None:\n",
    "    DATA_URL = 'http://vis-www.cs.umass.edu/lfw/lfw-bush.zip'\n",
    "else:\n",
    "    DATA_URL = CUSTOM_DATA_URL\n",
    "\n",
    "_, dataset_dir = cs236781.download.download_data(out_path=DATA_DIR, url=DATA_URL, extract=True, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Dataset` object that will load the extraced images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:18.195898Z",
     "iopub.status.busy": "2022-03-19T14:31:18.195738Z",
     "iopub.status.idle": "2022-03-19T14:31:18.325325Z",
     "shell.execute_reply": "2022-03-19T14:31:18.324769Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "im_size = 64\n",
    "tf = T.Compose([\n",
    "    # Resize to constant spatial dimensions\n",
    "    T.Resize((im_size, im_size)),\n",
    "    # PIL.Image -> torch.Tensor\n",
    "    T.ToTensor(),\n",
    "    # Dynamic range [0,1] -> [-1, 1]\n",
    "    T.Normalize(mean=(.5,.5,.5), std=(.5,.5,.5)),\n",
    "])\n",
    "\n",
    "ds_gwb = ImageFolder(os.path.dirname(dataset_dir), tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's see what we got. You can run the following block multiple times to display a random subset of images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:18.327939Z",
     "iopub.status.busy": "2022-03-19T14:31:18.327675Z",
     "iopub.status.idle": "2022-03-19T14:31:19.154387Z",
     "shell.execute_reply": "2022-03-19T14:31:19.153744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 530 images in dataset folder.\n"
     ]
    }
   ],
   "source": [
    "_ = plot.dataset_first_n(ds_gwb, 50, figsize=(15,10), nrows=5)\n",
    "print(f'Found {len(ds_gwb)} images in dataset folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.156860Z",
     "iopub.status.busy": "2022-03-19T14:31:19.156707Z",
     "iopub.status.idle": "2022-03-19T14:31:19.188541Z",
     "shell.execute_reply": "2022-03-19T14:31:19.187946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x0, y0 = ds_gwb[0]\n",
    "x0 = x0.unsqueeze(0).to(device)\n",
    "print(x0.shape)\n",
    "\n",
    "test.assertSequenceEqual(x0.shape, (1, 3, im_size, im_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Nets (GANs)\n",
    "<a id=part3_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs, first proposed in a [paper](https://arxiv.org/pdf/1406.2661.pdf) by Ian Goodfellow in 2014 are today arguably the most popular type of generative model. GANs are currently producing state of the art\n",
    "results in generative tasks over many different domains.\n",
    "\n",
    "In a GAN model, two different neural networks compete against each other: A **generator** and a **discriminator**.\n",
    "\n",
    "- The Generator, which we'll denote as $\\Psi _{\\bb{\\gamma}} : \\mathcal{U} \\rightarrow \\mathcal{X}$, maps a latent-space variable\n",
    "$\\bb{u}\\sim\\mathcal{N}(\\bb{0},\\bb{I})$ to an instance-space variable $\\bb{x}$ (e.g. an image).\n",
    "Thus a parametric evidence distribution $p_{\\bb{\\gamma}}(\\bb{X})$ is generated,\n",
    "which we typically would like to be as close as possible to the real evidence distribution, $p(\\bb{X})$.\n",
    "\n",
    "- The Discriminator, $\\Delta _{\\bb{\\delta}} : \\mathcal{X} \\rightarrow [0,1]$, is a network which,\n",
    "given an instance-space variable $\\bb{x}$, returns the  probability that $\\bb{x}$ is real, i.e. that $\\bb{x}$\n",
    "was sampled from $p(\\bb{X})$ and not $p_{\\bb{\\gamma}}(\\bb{X})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/gan.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training GANs\n",
    "<a id=part3_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator is trained to generate \"fake\" instances which will maximally fool the\n",
    "discriminator into returning that they're real. Mathematically, the generator's parameters\n",
    "$\\bb{\\gamma}$ should be chosen such as to **maximize** the expression\n",
    "$$\n",
    "\\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (\\Delta _{\\bb{\\delta}}(\\Psi _{\\bb{\\gamma}} (\\bb{z}) )).\n",
    "$$\n",
    "\n",
    "The discriminator is trained to classify between real images, coming from the training set,\n",
    "and fake images generated by the generator.\n",
    "Mathematically, the discriminator's parameters $\\bb{\\delta}$ should be chosen such as to\n",
    "**maximize** the expression\n",
    "$$\n",
    "\\mathbb{E} _{\\bb{x} \\sim p(\\bb{X}) } \\log \\Delta _{\\bb{\\delta}}(\\bb{x})  \\, + \\,\n",
    "\\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (1-\\Delta _{\\bb{\\delta}}(\\Psi _{\\bb{\\gamma}} (\\bb{z}) )).\n",
    "$$\n",
    "\n",
    "These two competing objectives can thus be expressed as the following min-max optimization:\n",
    "$$\n",
    "\\min _{\\bb{\\gamma}} \\max _{\\bb{\\delta}} \\,\n",
    "\\mathbb{E} _{\\bb{x} \\sim p(\\bb{X}) } \\log \\Delta _{\\bb{\\delta}}(\\bb{x})  \\, + \\,\n",
    "\\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (1-\\Delta _{\\bb{\\delta}}(\\Psi _{\\bb{\\gamma}} (\\bb{z}) )).\n",
    "$$\n",
    "\n",
    "A key insight into GANs is that we can interpret the above maximum as the *loss* with respect to\n",
    "$\\bb{\\gamma}$:\n",
    "\n",
    "$$\n",
    "L({\\bb{\\gamma}}) =\n",
    "\\max _{\\bb{\\delta}} \\, \\mathbb{E} _{\\bb{x} \\sim p(\\bb{X}) } \\log \\Delta _{\\bb{\\delta}}(\\bb{x})  \\, + \\,\n",
    "  \\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (1-\\Delta _{\\bb{\\delta}}(\\Psi _{\\bb{\\gamma}} (\\bb{z}) )).\n",
    "$$\n",
    "\n",
    "This means that the generator's loss function trains together with the generator\n",
    "itself in an adversarial manner. In contrast, when training our VAE we used a fixed L2 norm\n",
    "as a data loss term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "<a id=part3_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now implement a Deep Convolutional GAN (DCGAN) model.\n",
    "See the DCGAN [paper](https://arxiv.org/pdf/1511.06434.pdf) for architecture ideas and tips for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `Discriminator` class in the `hw4/gan.py` module.\n",
    "If you wish you can reuse the `EncoderCNN` class from the VAE model as the first part of the Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.191943Z",
     "iopub.status.busy": "2022-03-19T14:31:19.191764Z",
     "iopub.status.idle": "2022-03-19T14:31:19.283698Z",
     "shell.execute_reply": "2022-03-19T14:31:19.283023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 3, 64, 64])\n",
      "torch.Size([1, 1])\n",
      "tensor([[0.2913]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import hw4.gan as gan\n",
    "\n",
    "dsc = gan.Discriminator(in_size=x0[0].shape).to(device)\n",
    "print(dsc)\n",
    "\n",
    "print(x0.shape)\n",
    "d0 = dsc(x0)\n",
    "print(d0.shape)\n",
    "print(d0)\n",
    "\n",
    "test.assertSequenceEqual(d0.shape, (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `Generator` class in the `hw4/gan.py` module.\n",
    "If you wish you can reuse the `DecoderCNN` class from the VAE model as the last part of the Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.287092Z",
     "iopub.status.busy": "2022-03-19T14:31:19.286724Z",
     "iopub.status.idle": "2022-03-19T14:31:19.357892Z",
     "shell.execute_reply": "2022-03-19T14:31:19.357368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): ConvTranspose2d(128, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "z_dim = 128\n",
    "gen = gan.Generator(z_dim, 4).to(device)\n",
    "print(gen)\n",
    "\n",
    "z = torch.randn(1, z_dim).to(device)\n",
    "xr = gen(z)\n",
    "print(xr.shape)\n",
    "\n",
    "test.assertSequenceEqual(x0.shape, xr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Implementation\n",
    "<a id=part3_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the discriminator's loss function.\n",
    "Based on the above we can flip the sign and say we want to update the \n",
    "Discriminator's parameters $\\bb{\\delta}$ so that they **minimize** the expression\n",
    "$$\n",
    "- \\mathbb{E} _{\\bb{x} \\sim p(\\bb{X}) } \\log \\Delta _{\\bb{\\delta}}(\\bb{x})  \\, - \\,\n",
    "\\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (1-\\Delta _{\\bb{\\delta}}(\\Psi _{\\bb{\\gamma}} (\\bb{z}) )).\n",
    "$$\n",
    "\n",
    "We're using the Discriminator twice in this expression;\n",
    "once to classify data from the real data distribution and\n",
    "once again to classify generated data.\n",
    "Therefore our loss should be computed based on these two terms.\n",
    "Notice that since the discriminator returns a probability, we can formulate the above as two cross-entropy losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GANs are notoriously diffucult to train.\n",
    "One common trick for improving GAN stability during training is to make the classification labels noisy for the discriminator. This can be seen as a form of regularization, to help prevent the discriminator from overfitting.\n",
    "\n",
    "We'll incorporate this idea into our loss function. Instead of labels being equal to 0 or 1, we'll make them\n",
    "\"fuzzy\", i.e. random numbers in the ranges $[0\\pm\\epsilon]$ and $[1\\pm\\epsilon]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `discriminator_loss_fn()` function in the `hw4/gan.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.360831Z",
     "iopub.status.busy": "2022-03-19T14:31:19.360646Z",
     "iopub.status.idle": "2022-03-19T14:31:19.391271Z",
     "shell.execute_reply": "2022-03-19T14:31:19.390679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.4809)\n"
     ]
    }
   ],
   "source": [
    "from hw4.gan import discriminator_loss_fn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "y_data = torch.rand(10) * 10\n",
    "y_generated = torch.rand(10) * 10\n",
    "\n",
    "loss = discriminator_loss_fn(y_data, y_generated, data_label=1, label_noise=0.3)\n",
    "print(loss)\n",
    "\n",
    "test.assertAlmostEqual(loss.item(), 6.4808731, delta=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the generator's parameters $\\bb{\\gamma}$ should **minimize** the expression\n",
    "$$\n",
    "-\\mathbb{E} _{\\bb{z} \\sim p(\\bb{Z}) } \\log (\\Delta _{\\bb{\\delta}}(\\Psi _{\\bb{\\gamma}} (\\bb{z}) ))\n",
    "$$\n",
    "\n",
    "which can also be seen as a cross-entropy term. This corresponds to \"fooling\" the discriminator; Notice that the gradient of the loss w.r.t $\\bb{\\gamma}$ using this expression also depends on $\\bb{\\delta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `generator_loss_fn()` function in the `hw4/gan.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.394411Z",
     "iopub.status.busy": "2022-03-19T14:31:19.394212Z",
     "iopub.status.idle": "2022-03-19T14:31:19.426163Z",
     "shell.execute_reply": "2022-03-19T14:31:19.425466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0223)\n"
     ]
    }
   ],
   "source": [
    "from hw4.gan import generator_loss_fn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "y_generated = torch.rand(20) * 10\n",
    "\n",
    "loss = generator_loss_fn(y_generated, data_label=1)\n",
    "print(loss)\n",
    "\n",
    "test.assertAlmostEqual(loss.item(), 0.0222969, delta=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "<a id=part3_6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling from a GAN is straightforward, since it learns to generate data from an isotropic Gaussian latent space distribution.\n",
    "\n",
    "There is an important nuance however. Sampling is required during the process of training the GAN, since\n",
    "we generate fake images to show the discriminator.\n",
    "As you'll seen in the next section,  in some cases we'll need our samples to have gradients (i.e., to be part of\n",
    "the Generator's computation graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Implement the `sample()` method in the `Generator` class within the `hw4/gan.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.428981Z",
     "iopub.status.busy": "2022-03-19T14:31:19.428812Z",
     "iopub.status.idle": "2022-03-19T14:31:19.538523Z",
     "shell.execute_reply": "2022-03-19T14:31:19.538037Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = gen.sample(5, with_grad=False)\n",
    "test.assertSequenceEqual(samples.shape, (5, *x0.shape[1:]))\n",
    "test.assertIsNone(samples.grad_fn)\n",
    "_ = plot.tensors_as_images(samples.cpu())\n",
    "\n",
    "samples = gen.sample(5, with_grad=True)\n",
    "test.assertSequenceEqual(samples.shape, (5, *x0.shape[1:]))\n",
    "test.assertIsNotNone(samples.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<a id=part3_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training GANs is a bit different since we need to train two models simultaneously, each with it's own separate loss function and optimizer. We'll implement the training logic as a function that handles one batch of data\n",
    "and updates both the discriminator and the generator based on it.\n",
    "\n",
    "As mentioned above, GANs are considered hard to train. To get some ideas and tips you can see this [paper](https://arxiv.org/pdf/1606.03498.pdf), this list of [\"GAN hacks\"](https://github.com/soumith/ganhacks) or just do it the hard way :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Implement the `train_batch` function in the `hw4/gan.py` module.\n",
    "2. Tweak the hyperparameters in the `part3_gan_hyperparams()` function within the `hw4/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.541730Z",
     "iopub.status.busy": "2022-03-19T14:31:19.541553Z",
     "iopub.status.idle": "2022-03-19T14:31:19.643285Z",
     "shell.execute_reply": "2022-03-19T14:31:19.642743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 16, 'z_dim': 128, 'data_label': 1, 'label_noise': 0.2, 'discriminator_optimizer': {'type': 'Adam', 'lr': 0.0002, 'betas': (0.5, 0.999)}, 'generator_optimizer': {'type': 'Adam', 'lr': 0.0002, 'betas': (0.5, 0.999)}}\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from hw4.answers import part3_gan_hyperparams\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Hyperparams\n",
    "hp = part3_gan_hyperparams()\n",
    "batch_size = hp['batch_size']\n",
    "z_dim = hp['z_dim']\n",
    "\n",
    "# Data\n",
    "dl_train = DataLoader(ds_gwb, batch_size, shuffle=True)\n",
    "im_size = ds_gwb[0][0].shape\n",
    "\n",
    "# Model\n",
    "dsc = gan.Discriminator(im_size).to(device)\n",
    "gen = gan.Generator(z_dim, featuremap_size=4).to(device)\n",
    "\n",
    "# Optimizer\n",
    "def create_optimizer(model_params, opt_params):\n",
    "    opt_params = opt_params.copy()\n",
    "    optimizer_type = opt_params['type']\n",
    "    opt_params.pop('type')\n",
    "    return optim.__dict__[optimizer_type](model_params, **opt_params)\n",
    "dsc_optimizer = create_optimizer(dsc.parameters(), hp['discriminator_optimizer'])\n",
    "gen_optimizer = create_optimizer(gen.parameters(), hp['generator_optimizer'])\n",
    "\n",
    "# Loss\n",
    "def dsc_loss_fn(y_data, y_generated):\n",
    "    return gan.discriminator_loss_fn(y_data, y_generated, hp['data_label'], hp['label_noise'])\n",
    "\n",
    "def gen_loss_fn(y_generated):\n",
    "    return gan.generator_loss_fn(y_generated, hp['data_label'])\n",
    "\n",
    "# Training\n",
    "checkpoint_file = 'checkpoints/gan'\n",
    "checkpoint_file_final = f'{checkpoint_file}_final'\n",
    "if os.path.isfile(f'{checkpoint_file}.pt'):\n",
    "    os.remove(f'{checkpoint_file}.pt')\n",
    "\n",
    "# Show hypers\n",
    "print(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "1. Implement the `save_checkpoint` function in the `hw4.gan` module. You can decide on your own criterion regarding whether to save a checkpoint at the end of each epoch.\n",
    "1. Run the following block to train. It will sample some images from your model every few epochs so you can see the progress.\n",
    "2. When you're satisfied with your results, rename the checkpoints file by adding `_final`. When you run the `main.py` script to generate your submission, the final checkpoints file will be loaded instead of running training. Note that your final submission zip will not include the `checkpoints/` folder. This is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.645997Z",
     "iopub.status.busy": "2022-03-19T14:31:19.645814Z",
     "iopub.status.idle": "2022-03-19T14:31:19.744150Z",
     "shell.execute_reply": "2022-03-19T14:31:19.743668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading final checkpoint file checkpoints/gan_final instead of training\n"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "import tqdm\n",
    "from hw4.gan import train_batch, save_checkpoint\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "if os.path.isfile(f'{checkpoint_file_final}.pt'):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    num_epochs = 0\n",
    "    gen = torch.load(f'{checkpoint_file_final}.pt', map_location=device,)\n",
    "    checkpoint_file = checkpoint_file_final\n",
    "\n",
    "try:\n",
    "    dsc_avg_losses, gen_avg_losses = [], []\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        # We'll accumulate batch losses and show an average once per epoch.\n",
    "        dsc_losses, gen_losses = [], []\n",
    "        print(f'--- EPOCH {epoch_idx+1}/{num_epochs} ---')\n",
    "\n",
    "        with tqdm.tqdm(total=len(dl_train.batch_sampler), file=sys.stdout) as pbar:\n",
    "            for batch_idx, (x_data, _) in enumerate(dl_train):\n",
    "                x_data = x_data.to(device)\n",
    "                dsc_loss, gen_loss = train_batch(\n",
    "                    dsc, gen,\n",
    "                    dsc_loss_fn, gen_loss_fn,\n",
    "                    dsc_optimizer, gen_optimizer,\n",
    "                    x_data)\n",
    "                dsc_losses.append(dsc_loss)\n",
    "                gen_losses.append(gen_loss)\n",
    "                pbar.update()\n",
    "\n",
    "        dsc_avg_losses.append(np.mean(dsc_losses))\n",
    "        gen_avg_losses.append(np.mean(gen_losses))\n",
    "        print(f'Discriminator loss: {dsc_avg_losses[-1]}')\n",
    "        print(f'Generator loss:     {gen_avg_losses[-1]}')\n",
    "        \n",
    "        if save_checkpoint(gen, dsc_avg_losses, gen_avg_losses, checkpoint_file):\n",
    "            print(f'Saved checkpoint.')\n",
    "            \n",
    "\n",
    "        samples = gen.sample(5, with_grad=False)\n",
    "        fig, _ = plot.tensors_as_images(samples.cpu(), figsize=(6,2))\n",
    "        IPython.display.display(fig)\n",
    "        plt.close(fig)\n",
    "except KeyboardInterrupt as e:\n",
    "    print('\\n *** Training interrupted by user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.746961Z",
     "iopub.status.busy": "2022-03-19T14:31:19.746629Z",
     "iopub.status.idle": "2022-03-19T14:31:19.982071Z",
     "shell.execute_reply": "2022-03-19T14:31:19.981373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Images Generated from best model:\n"
     ]
    }
   ],
   "source": [
    "# Plot images from best or last model\n",
    "if os.path.isfile(f'{checkpoint_file}.pt'):\n",
    "    gen = torch.load(f'{checkpoint_file}.pt', map_location=device)\n",
    "print('*** Images Generated from best model:')\n",
    "samples = gen.sample(n=15, with_grad=False).cpu()\n",
    "fig, _ = plot.tensors_as_images(samples, nrows=3, figsize=(6,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "<a id=part3_8></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw4/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:19.985317Z",
     "iopub.status.busy": "2022-03-19T14:31:19.985117Z",
     "iopub.status.idle": "2022-03-19T14:31:20.020449Z",
     "shell.execute_reply": "2022-03-19T14:31:20.019933Z"
    }
   },
   "outputs": [],
   "source": [
    "from cs236781.answers import display_answer\n",
    "import hw4.answers as answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Explain in detail why during training we sometimes need to maintain gradients when sampling from the GAN,\n",
    "and other times we don't. When are they maintained and why? When are they discarded and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:20.023332Z",
     "iopub.status.busy": "2022-03-19T14:31:20.023157Z",
     "iopub.status.idle": "2022-03-19T14:31:20.062337Z",
     "shell.execute_reply": "2022-03-19T14:31:20.061881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "We maintain the gradients when we train the generator and we discard the gradients when we train the discriminator.\n",
       "We do it because when we train the discriminator we don't want to backpropagate through the generator because it will lead\n",
       "to the generator learning to fool the discriminator (the generator learns how to minimize the discriminator loss)\n",
       "rather than learning to create good images. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(answers.part3_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "1. When training a GAN to generate images, should we decide to stop training solely based on the fact that  the Generator loss is below some threshold?\n",
    "Why or why not?\n",
    "\n",
    "2. What does it mean if the discriminator loss remains at a constant value while the generator loss decreases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:20.065419Z",
     "iopub.status.busy": "2022-03-19T14:31:20.065262Z",
     "iopub.status.idle": "2022-03-19T14:31:20.097466Z",
     "shell.execute_reply": "2022-03-19T14:31:20.096883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       " 1. No, because when we decide stop training only based on the generator loss it might be that the generator loss is low \n",
       " because it is fooling the discriminator but the discriminator loss is high which means that the generator not producing  \n",
       " quality images. Basically, the generator loss is low not because he is creating good images but because the discriminator \n",
       " can't distinguish between real images to fakes ones.\n",
       " \n",
       " 2. It means that the generator is able to fool the discriminator and because the discriminator loss is constant \n",
       " it also means that the generator is still learning and improving. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(answers.part3_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Compare the results you got when generating images with the VAE to the GAN results.\n",
    "What's the main difference and what's causing it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-19T14:31:20.100180Z",
     "iopub.status.busy": "2022-03-19T14:31:20.099996Z",
     "iopub.status.idle": "2022-03-19T14:31:20.132665Z",
     "shell.execute_reply": "2022-03-19T14:31:20.132146Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "As we look at the results we got from the two models we can see that the main difference between the models is that the \n",
       "images that are being generated by the GAN are sharper but with a lot of artifacts and the background is less coherent, \n",
       "while the images generated by the VAE model are more blurry, but the background looks more real but as we said blurry. \n",
       "The main reason for this is the way the 2 models calculate their loss. $\\\\$ \n",
       "In the GAN model, the loss of the generator is evaluated by the amount of images that fooled the discriminator. \n",
       "As a result, the generator tries to create image with specific features (such as facial features like eyes, \n",
       "mouth and nose) and ignores most of the background. This is why we get images with sharp faces but blurry background. $\\\\$ \n",
       "On the other hand, the VAE model tries to generate images to be as close as they can be to the original images. As \n",
       "explained in the previous part, the model minimizes the distance between the generated image and the original, which \n",
       "creates blurry images because of the \"Regression to the mean\" problem. $\\\\$ \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(answers.part3_q3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}