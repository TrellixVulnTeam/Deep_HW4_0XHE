{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "$ \\newcommand{\\mat}[1]{\\boldsymbol {#1}} $\n",
    "$ \\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top} $\n",
    "$ \\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}} $\n",
    "$ \\newcommand{\\vec}[1]{\\boldsymbol {#1}} $\n",
    "$ \\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top} $\n",
    "$ \\newcommand{\\rvar}[1]{\\mathrm {#1}} $\n",
    "$ \\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}} $\n",
    "$ \\newcommand{\\diag}{\\mathop{\\mathrm {diag}}} $\n",
    "$ \\newcommand{\\set}[1]{\\mathbb {#1}} $\n",
    "$ \\newcommand{\\cset}[1]{\\mathcal{#1}} $\n",
    "$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} $\n",
    "$ \\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}} $\n",
    "$ \\newcommand{\\bb}[1]{\\boldsymbol{#1}} $\n",
    "$ \\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]} $\n",
    "$ \\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}} $\n",
    "$ \\newcommand{\\given}[]{\\,\\middle\\vert\\,} $\n",
    "$ \\newcommand{\\DKL}[2]{\\cset{D}_{\\text{KL}}\\left(#1\\,\\Vert\\, #2\\right)} $\n",
    "$ \\newcommand{\\grad}[]{\\nabla} $\n",
    "$ \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} $\n",
    "\n",
    "# Part 4: Summary Questions\n",
    "<a id=part4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains summary questions about various topics from the course material.\n",
    "\n",
    "You can add your answers in new cells below the questions.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Clearly mark where your answer begins, e.g. write \"**Answer:**\" in the beginning of your cell.\n",
    "- Provide a full explanation, even if the question doesn't explicitly state so. We will reduce points for partial explanations!\n",
    "- This notebook should be runnable from start to end without any errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the meaning of the term \"receptive field\" in the context of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain and elaborate about three different ways to control the rate at which the receptive field grows from layer to layer. Compare them to each other in terms of how they combine input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Imagine a CNN with three convolutional layers, defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-18T10:23:33.575663Z",
     "iopub.status.busy": "2022-01-18T10:23:33.574300Z",
     "iopub.status.idle": "2022-01-18T10:23:34.850060Z",
     "shell.execute_reply": "2022-01-18T10:23:34.850542Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "cnn = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=4, out_channels=16, kernel_size=5, stride=2, padding=2),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=7, dilation=2, padding=3),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "cnn(torch.rand(size=(1, 3, 1024, 1024), dtype=torch.float32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the size (spatial extent) of the receptive field of each \"pixel\" in the output tensor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You have trained a CNN, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$, and $f_l(\\cdot;\\vec{\\theta}_l)$ is a convolutional layer (not including the activation function).\n",
    "\n",
    "  After hearing that residual networks can be made much deeper, you decide to change each layer in your network you used the following residual mapping instead $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)+\\vec{x}$, and re-train.\n",
    "\n",
    "  However, to your surprise, by visualizing the learned filters $\\vec{\\theta}_l$ you observe that the original network and the residual network produce completely different filters. Explain the reason for this."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CNNs Answers:\n",
    "1. Receptive fields are defined portion of space or spatial construct containing units that provide input to a set of\n",
    "units within a corresponding layer. The receptive field is defined by the filter size of a layer within a convolution neural network.\n",
    "2. After some research we did on the subject, we found out that there are a few ways to *increase* the receptive field: $\\\\$\n",
    "a. Add more convolutional layers (make the network deeper) $\\\\$\n",
    "b. Add pooling layers or higher stride convolutions (sub-sampling) $\\\\$\n",
    "c. Use dilated convolutions $\\\\$\n",
    "In addition, we found a way to calculate the recptive field of the model. Let's say we have 2 convoutional laters $f1, f2$,\n",
    "with kernel size $k$, stride $s$ and receptive field $r$, then:\n",
    "$r1 = s2 \\cdot r2 + (k2 - s2)$. So we can say that the kernel size and receptive field of the previous layer also affect the result. $\\\\$\n",
    "**Need to compare the different options to each other in terms of how they combine input features**\n",
    "3. **TODO: LATER** $\\\\$\n",
    "4. **TODO: LATER** $\\\\$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **True or false**: dropout must be placed only after the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After applying dropout with a drop-probability of $p$, the activations are scaled by $1/(1-p)$. Prove that this scaling is required in order to maintain the value of each activation unchanged in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dropout Answers:\n",
    "1. **false**. Dropout can also be placed before the activation function. **Maybe** in the case of ReLU there is no difference between before and after. $\\\\$\n",
    "2."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You're training a an image classifier that, given an image, needs to classify it as either a dog (output 0) or a hotdog (output 1). Would you train this model with an L2 loss? if so, why? if not, demonstrate with a numerical example. What would you use instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. After months of research into the origins of climate change, you observe the following result:\n",
    "\n",
    "<center><img src=\"https://sparrowism.soc.srcf.net/home/piratesarecool4.gif\" /></center>\n",
    "\n",
    "You decide to train a cutting-edge deep neural network regression model, that will predict the global temperature based on the population of pirates in `N` locations around the globe.\n",
    "You define your model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-18T10:23:34.854297Z",
     "iopub.status.busy": "2022-01-18T10:23:34.853519Z",
     "iopub.status.idle": "2022-01-18T10:23:34.884129Z",
     "shell.execute_reply": "2022-01-18T10:23:34.884654Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N = 42  # number of known global pirate hot spots\n",
    "H = 128\n",
    "mlpirate = nn.Sequential(\n",
    "    nn.Linear(in_features=N, out_features=H),\n",
    "    nn.Sigmoid(),\n",
    "    *[\n",
    "        nn.Linear(in_features=H, out_features=H),\n",
    "        nn.Sigmoid(),\n",
    "    ]*N,\n",
    "    nn.Linear(in_features=H, out_features=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training your model you notice that the loss reaches a plateau after only a few iterations.\n",
    "It seems that your model is no longer training.\n",
    "What is the most likely cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Referring to question 2 above: A friend suggests that if you replace the `sigmoid` activations with `tanh`, it will solve your problem. Is he correct? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Regarding the ReLU activation, state whether the following sentences are **true or false** and explain:\n",
    "    1. In a model using exclusively ReLU activations, there can be no vanishing gradients.\n",
    "    1. The gradient of ReLU is linear with its input when the input is positive.\n",
    "    1. ReLU can cause \"dead\" neurons, i.e. activations that remain at a constant value of zero."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Losses and Activation functions Answers:\n",
    "1. We would not use L2 loss. L2 Loss Function is used to minimize the error which is the sum of the all the squared\n",
    "differences between the true value and the predicted value. In our case, there are only 2 possible values: 1 and 0. **PROVIDE NUMERICAL EXAMPLE** $\\\\$\n",
    "Instead we will use the Binary Cross Entropy loss function a.k.a the Log loss $\\\\$\n",
    "2. The most likely cause is that he have a problem of vanishing gradient. As we learned from class, when using the sigmoid\n",
    "activation function, there is a chance to encounter the vanishing gradient problem, because the values of the derivative\n",
    "of sigmoid are between 0 to 0.25. Those are small values which may lead to the gradient becoming zero during backpropagation.$\\\\$\n",
    "3. The friend is not correct. As we know, for tanh there is also a chance for us to encounter the vanishing gradient problem.\n",
    "If our friend would have listend in class, he would have suggested us to use ReLU, which is \"immune\" to the vanishing gradient problem.$\\\\$\n",
    "4. i. **true**. ReLU has gradient 1 when input > 0, and zero otherwise. Thus, multiplying a bunch of ReLU derivatives together in the backprop equations has the nice property of being either 1 or 0. There is no “vanishing” or “diminishing” of the gradient. The gradient travels to the bottom layers either as is or it becomes exactly 0 on the way. $\\\\$\n",
    "ii. **false**. As stated above, the gradient of ReLU is a const 1 when the input is positive. ReLU function is linear\n",
    "with its input when the input is positive but not the derivative.$\\\\$\n",
    "iii. **true**. As we know, ReLU has a value of zero when the input is negative, so if the majority of the input or in\n",
    "the worst case scenario, the whole input is negative, we will get a constant function of zero (in the worst case). Let's\n",
    "examine a bit more this situation when most of the inputs to these ReLU neurons are in the negative range. $\\\\$\n",
    "When most of these neurons return output zero, the gradients fail to flow during backpropagation, and the weights are not updated. Ultimately a large part of the network becomes inactive, and it is unable to learn further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the difference between: stochastic gradient descent (SGD), mini-batch SGD and regular gradient descent (GD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding SGD and GD:\n",
    "    1. Provide at least two reasons for why SGD is used more often in practice compared to GD.\n",
    "    2. In what cases can GD not be used at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. You have trained a deep resnet to obtain SoTA results on ImageNet.\n",
    "While training using mini-batch SGD with a batch size of $B$, you noticed that your model converged to a loss value of $l_0$ within $n$ iterations (batches across all epochs) on average.\n",
    "Thanks to your amazing results, you secure funding for a new high-powered server with GPUs containing twice the amount of RAM.\n",
    "You're now considering to increase the mini-batch size from $B$ to $2B$.\n",
    "Would you expect the number of of iterations required to converge to $l_0$ to decrease or increase when using the new batch size? explain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For each of the following statements, state whether they're **true or false** and explain why.\n",
    "    1. When training a neural network with SGD, every epoch we perform an optimization step for each sample in our dataset.\n",
    "    1. Gradients obtained with SGD have less variance and lead to quicker convergence compared to GD.\n",
    "    1. SGD is less likely to get stuck in local minima, compared to GD.\n",
    "    1. Training  with SGD requires more memory than with GD.\n",
    "    1. Assuming appropriate learning rates, SGD is guaranteed to converge to a local minimum, while GD is guaranteed to converge to the global minimum.\n",
    "    1. Given a loss surface with a narrow ravine (high curvature in one direction): SGD with momentum will converge more quickly than Newton's method which doesn't have momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In tutorial 5 we saw an example of bi-level optimization in the context of deep learning, by embedding an optimization problem as a layer in the network.\n",
    "    1. **True or false**: In order to train such a network, the inner optimization problem must be solved with a descent based method (such as SGD, LBFGS, etc).\n",
    "  Provide a mathematical justification for your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. You have trained a neural network, where each layer $l$ is represented by the mapping $\\vec{y}_l=f_l(\\vec{x};\\vec{\\theta}_l)$ for some arbitrary parametrized functions $f_l(\\cdot;\\vec{\\theta}_l)$.\n",
    "  Unfortunately while trying to break the record for the world's deepest network, you discover that you are unable to train your network with more than $L$ layers.\n",
    "    1. Explain the concepts of \"vanishing gradients\", and \"exploding gradients\".\n",
    "    2. How can each of these problems be caused by increased depth?\n",
    "    3. Provide a numerical example demonstrating each.\n",
    "    4. Assuming your problem is either of these, how can you tell which of them it is without looking at the gradient tensor(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimization Answers:\n",
    "1. In both gradient descent (GD) and stochastic gradient descent (SGD), we update a set of parameters in an iterative manner to minimize\n",
    "some error function. The difference is that in GD, we run through all the samples in the training set to do a single update,\n",
    " while in SGD, we use only one training sample. There is also a version of SGD where instead of one sample, we use a subset\n",
    " of the training set which is called Minibatch SGD.$\\\\$\n",
    "2. i. First of all, by using GD you need to store all the dataset in memory instead of storing a singular datapoint or subset of the dataset.$\\\\$\n",
    "Secondly, By using SGD instead of GD we get dynamic error surface. Because we compute an error surface for each batch and not for all the dataset. By\n",
    "doing so we might help the optimizer to get out of flat regions or sharp local minima (because they might not exist in the error surface of subsequent\n",
    "batches).$\\\\$\n",
    "Another advantage of using SGD is that it is faster than regular GD and will converge much faster compared to GD, because\n",
    "in each iteration we don't have to look at the whole training set which saves us time. $\\\\$\n",
    "ii. In some cases, GD can't be used. For example, cases where the data is arranged in a way that it poses a non-convex optimization problem,\n",
    "this is because gradient descent only works for problems which have a well defined convex optimization problem. Another\n",
    "example is when we don't have enough space to store all the dataset in memory, in this case, the network will fail. $\\\\$\n",
    "3. **TODO: LATER** $\\\\$\n",
    "4. i. **TODO: LATER** $\\\\$\n",
    "ii. **true**. As we already mentioned, SGD does converge quicker than regular GD. $\\\\$\n",
    "iii. **true**. In stochastic gradient descent the parameters are estimated for every observation, as opposed to the whole\n",
    "sample in regular gradient descent. This is what gives it a lot of randomness. The path of stochastic gradient descent\n",
    "wanders over more places, and thus is more likely to \"jump out\" of a local minimum, and find a global minimum. $\\\\$\n",
    "iv. **false**. As we mentioned already, GD requires more memory because we need to store the whole dataset. $\\\\$\n",
    "v. **false**. Both can converge to a global or to a local minima, however the chance for SGD to converge to global minima\n",
    "is higher. $\\\\$\n",
    "vi. **TODO: LATER** $\\\\$\n",
    "5. **TODO: LATER** $\\\\$\n",
    "6. i. Vanishing Gradient occurs when the derivative or slope will get smaller and smaller as we go backward with every layer during backpropagation.\n",
    "When weights update is very small or exponential small, the training time becomes much longer, and in the worst case, this may completely stop the neural network training. $\\\\$\n",
    "Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer\n",
    "during backpropagation. This situation is the exact opposite of the vanishing gradients. $\\\\$\n",
    "ii. Vanishing gradient occurs typically with the sigmoid and tanh activation functions because their derivatives are between\n",
    "0 to 0.25 and 0 to 1 respectively. Therefore, the updated weight values are small, and the new weight values are very similar to the old weight values.\n",
    "As we add more and more layers, the chance for the values of the derivatives to become zero during backpropagation increases. $\\\\$\n",
    "In contrast to vanishing gradients, exploding gradients problem happens not because of the activation function, but because\n",
    "of the weights. If the weights are high, it will cause the value of the derivative to also increase, which will result in the\n",
    "new updated weights being much more varied from the previous weights, and the gradient to never converge. $\\\\$\n",
    "iii. **TODO: LATER** $\\\\$\n",
    "iv. We can try and find the cause for the problem. For example, if we'll find out that: $\\\\$\n",
    "a. The parameters of the higher layers change significantly whereas the parameters of lower layers do not change much (or not at all) $\\\\$\n",
    "b. The model weights become 0 during training. $\\\\$\n",
    "c. The model learns very slowly and perhaps the training stagnates at a very early stage just after a few iterations $\\\\$\n",
    "Then, the problem is vanishing gradient. $\\\\$\n",
    "If however, we find out that: $\\\\$\n",
    "a. There is an exponential growth in the model parameters. $\\\\$\n",
    "b. The model weights become NaN during training. $\\\\$\n",
    "c. The model experiences  avalanche learning. $\\\\$\n",
    "Then, the problem is exploding gradient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You wish to train the following 2-layer MLP for a binary classification task:\n",
    "  $\\hat{y}^{(i)} =\\mat{W}_2~ \\varphi(\\mat{W}_1 \\vec{x}^{(i)}+ \\vec{b}_1) + \\vec{b}_2$\n",
    "  $\\\\$Your wish to minimize the in-sample loss function is defined as\n",
    "\n",
    "  $L_{\\mathcal{S}} = \\frac{1}{N}\\sum_{i=1}^{N}\\ell(y^{(i)},\\hat{y}^{(i)}) + \\frac{\\lambda}{2}\\left(\\norm{\\mat{W}_1}_F^2 + \\norm{\\mat{W}_2}_F^2 \\right)$\n",
    "\n",
    "  Where the pointwise loss is binary cross-entropy:\n",
    "\n",
    "  $\\ell(y, \\hat{y}) =  - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$\n",
    "\n",
    "  \n",
    "  Write an analytic expression for the derivative of the final loss $L_{\\mathcal{S}}$ w.r.t. each of the following tensors: $\\mat{W}_1$, $\\mat{W}_2$, $\\mat{b}_1$, $\\mat{b}_2$, $\\mat{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given the following code snippet, implement the custom backward function `part4_affine_backward` in `hw4/answers.py` so that it passes the `assert`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-18T10:23:34.893783Z",
     "iopub.status.busy": "2022-01-18T10:23:34.892985Z",
     "iopub.status.idle": "2022-01-18T10:23:35.017838Z",
     "shell.execute_reply": "2022-01-18T10:23:35.017408Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "from hw4.answers import part4_affine_backward\n",
    "\n",
    "N, d_in, d_out = 100, 11, 7\n",
    "dtype = torch.float64\n",
    "X = torch.rand(N, d_in, dtype=dtype)\n",
    "W = torch.rand(d_out, d_in, requires_grad=True, dtype=dtype)\n",
    "b = torch.rand(d_out, requires_grad=True, dtype=dtype)\n",
    "\n",
    "def affine(X, W, b):\n",
    "    return 0.5 * X @ W.T + b\n",
    "\n",
    "class AffineLayerFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, W, b):\n",
    "        result = affine(X, W, b)\n",
    "        ctx.save_for_backward(X, W, b)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return part4_affine_backward(ctx, grad_output)\n",
    "\n",
    "l1 = torch.sum(AffineLayerFunction.apply(X, W, b))\n",
    "l1.backward()\n",
    "W_grad1 = W.grad\n",
    "b_grad1 = b.grad\n",
    "\n",
    "l2 = torch.sum(affine(X, W, b))\n",
    "W.grad = b.grad = None\n",
    "l2.backward()\n",
    "W_grad2 = W.grad\n",
    "b_grad2 = b.grad\n",
    "\n",
    "assert torch.allclose(W_grad1, W_grad2)\n",
    "assert torch.allclose(b_grad1, b_grad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backpropagation Answers:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Regarding word embeddings:\n",
    "    1. Explain this term and why it's used in the context of a language model.\n",
    "    1. Can a language model like the sentiment analysis example from the tutorials be trained without an embedding (i.e. trained directly on sequences of tokens)? If yes, what would be the consequence for the trained model? if no, why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Considering the following snippet, explain:\n",
    "    1. What does `Y` contain? why this output shape?\n",
    "    2. How you would implement `nn.Embedding` yourself using only torch tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-18T10:23:35.021821Z",
     "iopub.status.busy": "2022-01-18T10:23:35.021337Z",
     "iopub.status.idle": "2022-01-18T10:23:35.191309Z",
     "shell.execute_reply": "2022-01-18T10:23:35.192511Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "X = torch.randint(low=0, high=42, size=(5, 6, 7, 8))\n",
    "embedding = nn.Embedding(num_embeddings=42, embedding_dim=42000)\n",
    "Y = embedding(X)\n",
    "print(f\"{Y.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regarding truncated backpropagation through time (TBPTT) with a sequence length of $S$: State whether the following sentences are **true or false**, and explain.\n",
    "    1. TBPTT uses a modified version of the backpropagation algorithm.\n",
    "    2. To implement TBPTT we only need to limit the length of the sequence provided to the model to length $S$.\n",
    "    3. TBPTT allows the model to learn relations between input that are at most $S$ timesteps apart."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sequence models Answers:\n",
    "1. i. Word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
    "For example: person and human would have a similar respresentation, where peace and war would not. Let's also remind ourselves\n",
    "the example from class, where we could do what is called \"arithmetics of word embedding\". For example:\n",
    "King - Man + Woman = Queen. $\\\\$ **ADD WHY IT IS USED IN LANGUAGE MODEL** $\\\\$\n",
    "ii. **TODO: LATER** $\\\\$\n",
    "2. **TODO: LATER**\n",
    "3. i. **I think it's true but i don't know for sure** $\\\\$\n",
    "ii. **TODO: LATER** $\\\\$\n",
    "iii. **TODO: LATER** $\\\\$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In tutorial 7 (part 2) we learned how to use attention to perform alignment between a source and target sequence in machine translation.\n",
    "    1. Explain qualitatively what the addition of the attention mechanism between the encoder and decoder does to the hidden states that the encoder and decoder each learn to generate (for their language). How are these hidden states different from the model without attention?\n",
    "  \n",
    "  2. After learning that self-attention is gaining popularity thanks to the shiny new transformer models, you decide to change the model from the tutorial: instead of the queries being equal to the decoder hidden states, you use self-attention, so that the keys, queries and values are all equal to the encoder's hidden states (with learned projections). What influence do you expect this will have on the learned hidden states?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. As we have seen, a variational autoencoder's loss is comprised of a reconstruction term and  a KL-divergence term. While training your VAE, you accidentally forgot to include the KL-divergence term.\n",
    "What would be the qualitative effect of this on:\n",
    "\n",
    "    1. Images reconstructed by the model during training ($x\\to z \\to x'$)?\n",
    "    1. Images generated by the model ($z \\to x'$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding VAEs, state whether each of the following statements is **true or false**, and explain:\n",
    "    1. The latent-space distribution generated by the model for a specific input image is $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "    2. If we feed the same image to the encoder multiple times, then decode each result, we'll get the same reconstruction.\n",
    "    3. Since the real VAE loss term is intractable, what we actually minimize instead is it's upper bound, in the hope that the bound is tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regarding GANs, state whether each of the following statements is **true or false**, and explain:\n",
    "    1. Ideally, we want the generator's loss to be low, and the discriminator's loss to be high so that it's fooled well by the generator.\n",
    "    2. It's crucial to backpropagate into the generator when training the discriminator.\n",
    "    3. To generate a new image, we can sample a latent-space vector from $\\mathcal{N}(\\vec{0},\\vec{I})$.\n",
    "    4. It can be beneficial for training the generator if the discriminator is trained for a few epochs first, so that it's output isn't arbitrary.\n",
    "     5. If the generator is generating plausible images and the discriminator reaches a stable state where it has 50% accuracy (for both image types), training the generator more will further improve the generated images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You have implemented a graph convolutional layer based on the following formula, for a graph with $N$ nodes:\n",
    "$\n",
    "\\mat{Y}=\\varphi\\left( \\sum_{k=1}^{q} \\mat{\\Delta}^k \\mat{X} \\mat{\\alpha}_k + \\vec{b} \\right).\n",
    "$\n",
    "    1. Assuming $\\mat{X}$ is the input feature matrix of shape $(N, M)$: what does $\\mat{Y}$ contain in it's rows?\n",
    "    1. Unfortunately, due to a bug in your calculation of the Laplacian matrix, you accidentally zeroed the row and column $i=j=5$ (assume more than 5 nodes in the graph).\n",
    "What would be the effect of this bug on the output of your layer, $\\mat{Y}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We have discussed the notion of a Receptive Field in the context of a CNN. How would you define a similar concept in the context of a GCN (i.e. a model comprised of multiple graph convolutional layers)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}